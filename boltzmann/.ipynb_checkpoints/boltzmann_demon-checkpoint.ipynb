{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set configuration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from common.config import *\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Atari Environment </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_id = \"DemonAttack-v0\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><hr></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, tau):\n",
    "        state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "        q_value = self.forward(state)\n",
    "        q_value = torch.nn.functional.softmax(q_value/tau)\n",
    "        qv = q_value.data.cpu().numpy()[0]\n",
    "        action_val = np.random.choice(qv, p=qv)\n",
    "        action = np.argmax(action_val == qv)\n",
    "        return int(action)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "        q_value = self.forward(state)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)\n",
    "\n",
    "replay_initial = cfg.REPLAY_INIT\n",
    "replay_buffer = ReplayBuffer(cfg.REPLAY_BUFFER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Boltzmann exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c91cf28>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHm9JREFUeJzt3Xl0XXW99/H39wyZ0wxN2iZN23QAOtExLS0ICAhUQFEe5KKg8KCiiI/oc5/rBe991tW77r36uFheZDlRBkVBqkyCVUEGsYwtKW2hpS2d5zbpkDRt5uT3/HF2IS1Jm+ScnJ2zz+e11llnn9/Z5/y+Ozv5nJ3f2YM55xARkWAJ+V2AiIgknsJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBFAkmZ2VlJS4ysrKZHYpIpLyli9fvt85V9qX1yQ13CsrK6murk5mlyIiKc/MtvX1NRqWEREJIIW7iEgAKdxFRAJI4S4iEkAKdxGRADpluJvZA2ZWY2aru7QVm9lzZrbBuy8a2DJFRKQverPl/itgwQlttwMvOOdOA17wHouIyCBxynB3zi0BDp7QfCXwoDf9IPCpBNd1nKdW7uKhN/q8m6eISNrq75j7cOfcHgDvflhPM5rZzWZWbWbVtbW1/ers2TV7+flLm/pXqYhIGhrwL1Sdcwudc1XOuarS0j4dPfu+OZXF7KprYnddU4KrExEJpv6G+z4zKwPw7msSV9KHzaksBuDNrSeODomISHf6G+5PAzd40zcATyWmnO5NKhtCfmaEZVsU7iIivdGbXSEfAV4HzjCznWb2ReAHwMVmtgG42Hs8YMIhY3ZlkcJdRKSXTnlWSOfcZ3t46qIE13JScyqLeWn9eg4dbaUoNyOZXYuIpJyUOUJ17liNu4uI9FbKhPu0igIyIiGFu4hIL6RMuGdGwsyoKGTZ1kN+lyIiMuilTLgDzBlbxOpd9Rxtafe7FBGRQS2lwn3u2KF0dDpWbK/zuxQRkUEtpcJ91uhCQgbLNO4uInJSKRXu+VlRJpcP4U3t7y4iclIpFe4Q2999xY5DtLZ3+l2KiMiglXLhPm/cUJrbOlm1U+PuIiI9Sb1wHzsUM3h90wG/SxERGbRSLtwLcqJMKR/Ca5v2+12KiMiglXLhDnD2+BLe2lZHc1uH36WIiAxKKRnu88cNpbWjk+XbdLSqiEh3UjLc54wtJhwyjbuLiPQgJcM9LzPC9IoCjbuLiPQgJcMdYP74oazaWc8RnWdGRORDUjbczx5fQken09GqIiLdSNlwnz2miIxwiNc3a9xdROREKRvuWdEws8YUatxdRKQbKRvuAPPHlbBm92HqGlv9LkVEZFBJ6XA/e8JQnIM3NDQjInKclA73GaMKyc0Is2SDhmZERLpK6XCPhkPMH1/Ckvdqcc75XY6IyKCR0uEOcP7pJew81MTWA41+lyIiMmikfLifd3opAC9vqPW5EhGRwSPlw33M0FxGF+ew5D2Fu4jIMSkf7gDnnV7C65sO6NJ7IiKeQIT7uaeVcrS1g7e26xTAIiIQkHA/e/xQIiHT0IyIiCcQ4Z6fFWXW6CJe1v7uIiJAQMId4NzTSli9u54DR1r8LkVExHdxhbuZfcvM1pjZajN7xMyyElVYX513einOwSsbtfUuItLvcDezkcA3gCrn3FQgDFybqML6aurIAopyovx9vcbdRUTiHZaJANlmFgFygN3xl9Q/4ZBx/umlvPReLR2dOhWBiKS3foe7c24XcCewHdgD1Dvn/nrifGZ2s5lVm1l1be3AblVfOGk4B4+2smpn3YD2IyIy2MUzLFMEXAmMBcqBXDO7/sT5nHMLnXNVzrmq0tLS/lfaC+efVko4ZLy4tmZA+xERGeziGZb5GLDFOVfrnGsDngDOTkxZ/VOQE6VqTBEvrFO4i0h6iyfctwPzzCzHzAy4CFibmLL676JJw1i75zC765r8LkVExDfxjLkvBR4D3gLe8d5rYYLq6rcLJw4H4EVtvYtIGotrbxnn3L855yY656Y65z7vnPP9CKLxpbmMGZqjcBeRtBaYI1SPMTMunDiMVzfup6m1w+9yRER8EbhwB7ho4nBa2jt5bZOOVhWR9BTIcJ87tpjcjLD2mhGRtBXIcM+IhDj3tFJeWLuPTh2tKiJpKJDhDnDJlOHsO9zC27vq/S5FRCTpAhvuF00cTiRkPLN6r9+liIgkXWDDvSAnyvzxQ3l2zV6c09CMiKSXwIY7wCVTRrBl/1E21BzxuxQRkaQKdLhfOnk4ZvCshmZEJM0EOtyHDcli5qhCnlmjcBeR9BLocAdYMHUEa3YfZsfBRr9LERFJmsCH+6VTRgDwrLbeRSSNBD7cxwzNZeKIfIW7iKSVwIc7xLbeq7cdoqah2e9SRESSIi3C/bIzy3BOe82ISPpIi3A/Y0Q+pw3L44+r9vhdiohIUqRFuAN8Yno5y7YeZE+9Lr8nIsGXNuF+xbQyAP70trbeRST40ibcx5XmMaV8CH9UuItIGkibcIfY0MyqHXVsP6ADmkQk2NIq3C8/MzY0s/id3T5XIiIysNIq3EcV5zBzdKH2mhGRwEurcAf4xLRy1u45zEadBlhEAiztwv3yaWWYwdOrNDQjIsGVduE+fEgW54wv4ckVO3WFJhEJrLQLd4CrZo1kx8Emqrcd8rsUEZEBkZbhfumUEeRkhHl8+U6/SxERGRBpGe65mREWTB3Bn97eQ3Nbh9/liIgkXFqGO8DVsypoaGnnuXf3+V2KiEjCpW24zxs3lPKCLB5/S0MzIhI8aRvuoZDxqZkjWfJerS7iISKBE1e4m1mhmT1mZuvMbK2ZzU9UYclw1awKOh08vVL7vItIsMS75f5j4Bnn3ERgOrA2/pKSZ8KwPKaPKuTRau3zLiLB0u9wN7MhwHnA/QDOuVbnXF2iCkuWa6oqWL+vgZU7Uq50EZEexbPlPg6oBX5pZivM7D4zyz1xJjO72cyqzay6trY2ju4Gxienl5OTEWbRsh1+lyIikjDxhHsEmAX83Dk3EzgK3H7iTM65hc65KudcVWlpaRzdDYz8rCifmFbO06t209Dc5nc5IiIJEU+47wR2OueWeo8fIxb2KeezZ42mqa1DJxMTkcDod7g75/YCO8zsDK/pIuDdhFSVZNMrCpg4Ip9Hlm33uxQRkYSId2+Z/wU8bGZvAzOA/4q/pOQzMz531mhW7zrM6l31fpcjIhK3uMLdObfSG0+f5pz7lHMuZU+zeOWMkWRFQ9p6F5FASNsjVE9UkB3lsjPLeGrlbo62tPtdjohIXBTuXVx31miOtLTz5IpdfpciIhIXhXsXs0YXMXXkEH79+lYdsSoiKU3h3oWZccP8St7bd4TXNx3wuxwRkX5TuJ/gE9PLKc7N4FevbfW7FBGRflO4nyArGubaOaN4fu0+dh5q9LscEZF+Ubh34/p5YzAzfvPGNr9LERHpF4V7N8oLs7lk8nB+9+YOXWNVRFKSwr0HN5xdSV1jG3/QbpEikoIU7j04a2wxk8uGcO/Lm+ns1G6RIpJaFO49MDO+cv44NtUe5cV1NX6XIyLSJwr3k7jszDJGFmazcMlmv0sREekThftJRMMhvviRsSzbepC3tqfsOdFEJA0p3E/hH+aMoiA7ysK/a+tdRFKHwv0UcjMjfH7eGJ59dy+ba4/4XY6ISK8o3HvhhrMriYZD3PvyFr9LERHpFYV7L5TmZ3L17AoeX76TvfXNfpcjInJKCvdeuuX88XQ6xy/+vsnvUkRETknh3kujinO4atZIHlm2nZrD2noXkcFN4d4Ht14wgfZOxz3a711EBjmFex+MGZrLp2aM5OGl26htaPG7HBGRHinc++jrF06gtb2ThUs09i4ig5fCvY/GluRy5YyRPPTGdvYf0da7iAxOCvd++PqFE2hp7+Bnf9PWu4gMTgr3fhhfmsdnZo/ioTe26VJ8IjIoKdz76baPnQYG//3cBr9LERH5EIV7P5UXZnPj2ZU8sWIn6/c2+F2OiMhxFO5xuOX88eRlRLjzr+v9LkVE5DgK9zgU5WbwlfPH8dy7+1i+Ted7F5HBQ+Eep5s+MpaSvEy+/+e1OKdrrYrI4KBwj1NORoR/vOR0qrcdYvHbe/wuR0QESEC4m1nYzFaY2eJEFJSKrqkaxaSyIfzgL+tobuvwuxwRkYRsud8GrE3A+6SscMj4t09MZlddE/fqpGIiMgjEFe5mVgFcDtyXmHJS17xxQ/n41BH87KVNuqCHiPgu3i33u4BvA509zWBmN5tZtZlV19bWxtnd4PadyybR4Rw/fGad36WISJrrd7ib2RVAjXNu+cnmc84tdM5VOeeqSktL+9tdShhVnMOXPjKWJ1bsYvm2g36XIyJpLJ4t93OAT5rZVmARcKGZPZSQqlLYrRdMoLwgi+88sZq2jh7/oRERGVD9Dnfn3B3OuQrnXCVwLfCic+76hFWWonIzI3zvyqms39fA/a9s8bscEUlT2s99AFw8eTgXTx7OXc+/x46DOmukiCRfQsLdOfeSc+6KRLxXUHz3k1MImfHdp9foyFURSTptuQ+QkYXZfOtjp/PCuhqeXbPX73JEJM0o3AfQjedUMqlsCP/3qTXUNbb6XY6IpBGF+wCKhkPc+ZlpHDraynefXuN3OSKSRhTuA2xKeQG3XjCBP6zczV81PCMiSaJwT4JbL5jApLIhfOfJ1Rw6quEZERl4CvckyIjEhmfqGlv5Nw3PiEgSKNyTZEp5AV+/cAJPr9rNH1ft9rscEQk4hXsS3XrBBGaMKuQ7T7yjg5tEZEAp3JMoGg5x97UzccBti1bQrnPPiMgAUbgn2eihOfznp6fy1vY67n5hg9/liEhAKdx9cOWMkfyPWRX85G8bWbr5gN/liEgAKdx98r0rpzC6OIdvLFpBbUOL3+WISMAo3H2SlxnhZ9fNpr6pja//9i2Nv4tIQincfTS5fAjfv+pMlm45yA+fXe93OSISIAp3n316ZgVfmD+GhUs28+d39vhdjogEhMJ9EPjXyyczc3Qh//ToKt7b1+B3OSISAAr3QSAjEuLn180mJzPCTb96kwNH9AWriMRH4T5IjCjI4t4vVFHb0MLNv1lOc1uH3yWJSApTuA8iM0YV8qNrZrB82yH++fG3dXk+Eek3hfsgc/m0Mv7PJafz1Mrd3P3CRr/LEZEUFfG7APmwWy+YwOb9R/nv599jREEm/zBntN8liUiKUbgPQmbGD66axv4jrdzxxDsUZGewYOoIv8sSkRSiYZlBKiMS4hfXz2JaRSHfWLSC1zfpHDQi0nsK90EsJyPCL2+cw+jiHL7862pW76r3uyQRSREK90GuKDeD33xxLgXZUT5//1Le3X3Y75JEJAUo3FNAWUE2D3/pLLKiYa677w3W7lHAi8jJKdxTRGVJLo98eR6ZkTDX3beUdXsV8CLSM4V7CqksyWXRzfPICIf43L0KeBHpmcI9xVSW5PKIF/DX/OJ1lm876HdJIjIIKdxT0NiSXB67ZT5D8zK57r6lvLS+xu+SRGSQUbinqIqiHB796nzGl+bxpQereXrVbr9LEpFBpN/hbmajzOxvZrbWzNaY2W2JLExOrSQvk0dunsfsMUXctmgF9728WScbExEgvi33duAfnXOTgHnArWY2OTFlSW8NyYry4E1zWTBlBP/xp7V858nVtOl6rCJpr9/h7pzb45x7y5tuANYCIxNVmPReVjTMTz83i699dDyPLNvOjb9cRn1jm99liYiPEjLmbmaVwExgaTfP3Wxm1WZWXVtbm4jupBuhkPHtBRO58zPTWbblIFf9/FU21x7xuywR8Unc4W5mecDjwDedcx/a8do5t9A5V+WcqyotLY23OzmFq2dX8NAXz+Lg0Vau/MmrPLN6r98liYgP4gp3M4sSC/aHnXNPJKYkiddZ44ay+BvnMq40l68+tJzv/2Ut7RqHF0kr8ewtY8D9wFrn3I8SV5IkwsjCbH7/1flcP2809/x9M9ffv5Sahma/yxKRJIlny/0c4PPAhWa20rtdlqC6JAEyI2H+41Nn8qNrprNyRx0L7nqZ597d53dZIpIE/b4Sk3PuFcASWIsMkKtmVTCtooDbFq3ky7+u5nNnjeZfL59EToYuxCUSVDpCNU1MGJbPk187h6+cP45Hlm3nirtfYdWOOr/LEpEBonBPIxmREHd8fBIPf+ksmto6+PTPXuU///QuTa0dfpcmIgmmcE9DZ48v4dlvnce1c0dz78tbuPSuJby2cb/fZYlIAinc09SQrCj/9ekzWXTzPEIGn7tvKf/06Cr2H2nxuzQRSQCFe5qbN24oz3zzPL56/nieXLGLC+58iQde2aLz04ikOIW7kBUNc/vHJ/LMN89jxqhC/n3xu1z245d5VUM1IilL4S7vmzAsj1/fNJd7v1BFS3sn1923lJt+9aYuyC2SghTuchwz4+LJw/nrt87jnxdMpHrrQS67+2W+uWgF2w80+l2eiPSSJfPiDlVVVa66ujpp/Un86hvbuGfJJh54dQvtHY5r547ilo9OYGRhtt+liaQNM1vunKvq02sU7tIbNYeb+fELG/h99Q6cg0/PHMlXPzqe8aV5fpcmEngKdxlwu+qauHfJZha9uZ2W9k4um1rGLR8dz9SRBX6XJhJYCndJmv1HWnjglS385vVtNLS0M7eymBvPqeSSycOJhPVVjkgiKdwl6eqb2ni0egcPvr6VHQebKC/I4vr5Y7h2zmiKczP8Lk8kEBTu4puOTseL62r41WtbeHXjATLCIS6ZMpxrqkZxzoQSwiGdQFSkv/oT7jrnqyREOBTbhfLiycN5b18Dv126nT+s3MXit/dQXpDF1bMruHr2KEYPzfG7VJG0oC13GTAt7R08/24Nv6/ewZINtTgHM0cXcsW0ci4/s4wRBVl+lyiSEjQsI4PW7romnlwR25Jfu+cwZjBnTDFXTC9jwdQRDMtX0Iv0ROEuKWFT7REWr9rD4rd3s6HmCADTRxXysYnDuGjScCaV5RO7RK+IgMJdUtD6vQ38dc1enl9X8/6VocoLsrhw0jAuOGMYc8cWk58V9blKEX8p3CWl1TQ089K6Wp5fu49XNu6nsbWDcMiYXlHAORNKOHt8CbPGFJIZCftdqkhSKdwlMJrbOnhr+yFe23iAVzbu5+2ddXQ6yIqGqBpTzOwxRVRVFjFjVKG27CXwFO4SWIeb21i6+SCvbtzPG5sPsH5fA86BGZwxPJ+qyiJmjylixqgixhTnENJ+9RIgCndJGw3NbazcUUf11kO8tf0QK7bXcaSlHYC8zAiTy4cwtbyAMyti9+NK83QglaQsHcQkaSM/K8q5p5Vy7mmlQOwI2fV7G3hnVx2rdx1m9e56frtsG82vxi4XmB0Nc/qIfE4flsfpw/OZMDx2X16QpT1zJJC05S6B1d7Ryeb9R1m9q553dtWzbk8DG2qOHHcR8NyMMBOGx0J/XGkeY4bmeLdc8jK17SODg4ZlRHrh0NFWNtQc4b19DWzY1+BNHx/6ACV5me+HfeXQXEYX51BemE1ZQRYjCrKI6uyXkiQalhHphaLcDOaOLWbu2OLj2hua29h2oJHtBxvZeuAo2/Y3su3gUV7fdIAn3tp13LxmMCw/k7KCbMoLsygvyKasMJvygiyGDcmkNC+L0vxMsjO026b4Q+Eu4snPijJ1ZEG3Fx5pbutg56FGdtc1s6e+iV11zeypa2JPfTPr9jTw4roamts6P/S6vMwIJXkZlOZnUpqfSUleJqV5seni3AwKczIoyolSmJNBYU5U/w1IwijcRXohKxpmwrB8JgzL7/Z55xx1jW3srm+itqEldjsSu99/pJXahmbW723g1SMHqG9q67GfvMwIhTlRCnOiFOXEwr8wO0pRTpT8rCh5WRHyMiPkZUUYkhUhL7NLW2ZEewTJ+xTuIglgZhTlZlDUiwuUtLR3UNvQQl1jG4caWznU2Ea9d3+osZX6Lu07DzXF2pra6M3XY7kZ4S4fAFFyM8JkR8NkZYTJiYbJzvBu0TA5x56LhsnJiJCdESI7GiE7I/ZcZiRERiREZiRMRiRERjhENGzauyhFKNxFkiwzEqaiKIeKot6/prPT0djWwZHmdhqa22hoaedIcztHWrzH3vQHbe00tLTT1NrO4eY2Gls7aG7toKmtg8bWDlraPzyE1BtmEA2HyIzEbhnh2AfAiR8CH7TFHkfCRiQcIhIyIqHYh0Q41KUtbERDIcIhI+rNe2w6HAoRPWHe2HOxecJmhMwIhWLXFQgde2xdHh+bLwQhOzZ9wjxm3jSB+ACLK9zNbAHwYyAM3Oec+0FCqhKR44RC9v7QSyLOg9/Z6Whqi4V9U+sH942tHTR3aW/t6KS13bt1dNLS1kFLl7aW9hOeb++gtb2Txsb2959rae+ko9PR3tlJe6ejvcOb7nC0dyZvb72+MOP4Dw374EPi2AcCxKZDZliX+/en6dpmPHDDnKRerKbf4W5mYeCnwMXATuBNM3vaOfduoooTkYERChm5mRFyfd6X3znnBb936+ikrSPW1tbxwYfCiW1tHR98WMTeI3Ygm3OODu89nYu1dbrYraOTLtOOThf7kOvw2jq9tm5fc2y+Y/O42Ps7777TORzevdfe6ejS5siIJPfL8njW7Fxgo3NuM4CZLQKuBBTuItIrZuYN2fhdSfDE81EyEtjR5fFOr+04ZnazmVWbWXVtbW0c3YmISG/FE+7dfePwoQE059xC51yVc66qtLQ0ju5ERKS34gn3ncCoLo8rgN3xlSMiIokQT7i/CZxmZmPNLAO4Fng6MWWJiEg8+v2FqnOu3cy+DjxLbFfIB5xzaxJWmYiI9Ftc+0E55/4M/DlBtYiISILoLEUiIgGkcBcRCaCkXqzDzGqBbf18eQmwP4HlpAItc3rQMgdfvMs7xjnXp33Jkxru8TCz6r5eiSTVaZnTg5Y5+PxYXg3LiIgEkMJdRCSAUincF/pdgA+0zOlByxx8SV/elBlzFxGR3kulLXcREemllAh3M1tgZuvNbKOZ3e53PadiZqPM7G9mttbM1pjZbV57sZk9Z2YbvPsir93M7G5v+d42s1ld3usGb/4NZnZDl/bZZvaO95q7zbsuWE99JGm5w2a2wswWe4/HmtlSr5bfeecgwswyvccbvecru7zHHV77ejO7tEt7t78DPfWRLGZWaGaPmdk6b33PD/J6NrNveb/Tq83sETPLCuJ6NrMHzKzGzFZ3afNtvZ6sjx457yohg/VG7Lw1m4BxQAawCpjsd12nqLkMmOVN5wPvAZOBHwK3e+23A//Pm74M+Aux0yjPA5Z67cXAZu++yJsu8p5bBsz3XvMX4ONee7d9JGm5/zfwW2Cx9/j3wLXe9C+AW7zprwG/8KavBX7nTU/21m8mMNZb7+GT/Q701EcSl/lB4EvedAZQGNT1TOx6DVuA7C4/+xuDuJ6B84BZwOoubb6t1576OOkyJPMPoZ8/5PnAs10e3wHc4XddfVyGp4hdjnA9UOa1lQHrvel7gM92mX+99/xngXu6tN/jtZUB67q0vz9fT30kYRkrgBeAC4HF3i/hfiBy4nokdrK5+d50xJvPTly3x+br6XfgZH0kaZmHEAs7O6E9kOuZDy7QU+ytt8XApUFdz0Alx4e7b+u1pz5OVn8qDMv06opPg5X3r+hMYCkw3Dm3B8C7H+bN1tMynqx9ZzftnKSPgXYX8G2g03s8FKhzzrV3U+P7y+U9X+/N39efw8n6SIZxQC3wS4sNR91nZrkEdD0753YBdwLbgT3E1ttygr+ej/FzvfY5B1Mh3Ht1xafByMzygMeBbzrnDp9s1m7aXD/afWFmVwA1zrnlXZu7mdWd4rlU+zlEiP3r/nPn3EzgKLF/pXuSast3HG/890piQynlQC7w8W5mDdp6PpVkLE+fX5MK4Z6SV3wysyixYH/YOfeE17zPzMq858uAGq+9p2U8WXtFN+0n62MgnQN80sy2AouIDc3cBRSa2bHTSnet8f3l8p4vAA7S95/D/pP0kQw7gZ3OuaXe48eIhX1Q1/PHgC3OuVrnXBvwBHA2wV/Px/i5Xvucg6kQ7il3xSfvm+/7gbXOuR91eepp4Ng35jcQG4s/1v4F7xvxeUC99y/Zs8AlZlbkbTVdQmyscQ/QYGbzvL6+cMJ7ddfHgHHO3eGcq3DOVRJbPy86564D/gZc3U0tXWu82pvfee3XentZjAVOI/bFU7e/A95reupjwDnn9gI7zOwMr+ki4F0Cup6JDcfMM7Mcr55jyxvo9dyFn+u1pz56NtBfSiToi43LiO1xsgn4F7/r6UW9HyH2L9PbwErvdhmxscMXgA3efbE3vwE/9ZbvHaCqy3vdBGz0bv+zS3sVsNp7zU/44IC0bvtI4rJ/lA/2lhlH7I92I/AokOm1Z3mPN3rPj+vy+n/xlmk93h4EJ/sd6KmPJC7vDKDaW9d/ILZXRGDXM/A9YJ1X02+I7fESuPUMPELse4U2YlvNX/RzvZ6sj55uOkJVRCSAUmFYRkRE+kjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgA/X9VsKUeP/76HQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tau_start = cfg.TAU_START\n",
    "tau_final = cfg.TAU_FINAL\n",
    "tau_decay = cfg.TAU_DECAY\n",
    "\n",
    "tau_by_frame = lambda frame_idx: tau_final + (tau_start - tau_final) * math.exp(-1. * frame_idx / tau_decay)\n",
    "plt.plot([tau_by_frame(i) for i in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    p1 = np.exp(p) / np.sum(np.exp(p))\n",
    "    return -sum(p1*np.log(p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minzy/anaconda3/envs/pytorch/lib/python3.5/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_idx:    1927, (0.06 %)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7dd6aef1aed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mq_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_frames = cfg.NUM_FRAMES\n",
    "batch_size = cfg.BATCH_SIZE\n",
    "gamma      = cfg.GAMMA\n",
    "episode = 0\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state_traj = []\n",
    "q_value_traj = []\n",
    "\n",
    "episode_state = []\n",
    "episode_q_val = []\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    tau = tau_by_frame(frame_idx)\n",
    "    action = model.act(state, tau)\n",
    "    \n",
    "    if episode % 20 == 0:\n",
    "        q_values = model.predict(state).data.cpu().numpy()[0]\n",
    "\n",
    "        episode_state.append(state)\n",
    "        episode_q_val.append(q_values)\n",
    "        q_entropy = entropy(q_values)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        if episode % 20 == 0:\n",
    "            state_traj.append(episode_state)\n",
    "            q_value_traj.append(episode_q_val)\n",
    "        \n",
    "        episode += 1\n",
    "        episode_state = []\n",
    "        episode_q_val = []\n",
    "        episode_q_entropy = []\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    print('\\rframe_idx: %7d, (%.2f %%)' %(frame_idx, frame_idx/num_frames*100), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minzy/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type CnnDQN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "from common.save_file import *\n",
    "\n",
    "model_dir = cfg.MODEL_DIR\n",
    "var_dir = cfg.VAR_DIR\n",
    "name = \"boltzmann_\" + env_id\n",
    "\n",
    "save_model(model, model_dir, name)\n",
    "\n",
    "var_dict = {\n",
    "            \"all_rewards\": all_rewards,\n",
    "            \"losses\": losses,\n",
    "            \"state_traj\": state_traj,\n",
    "            \"q_value_traj\": q_value_traj\n",
    "           }\n",
    "\n",
    "save_variable(name, var_dir, var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
