{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cart Pole Environment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Epsilon greedy exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110ab03c8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Q Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, env.action_space.n)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-01b1b8d823a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "num_frames = 10000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><hr></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_frames = 1400000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
