{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math, random, time\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "from common.save_file import *\n",
    "from common.entropy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_id = \"DemonAttack-v0\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env, scale=True)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set configuration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from common.config import *\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><hr></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "    \n",
    "    def predict(self, state):\n",
    "        state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "        q_value = self.forward(state)\n",
    "        return q_value\n",
    "    \n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1 and classname.find('Layer') == -1:\n",
    "            nn.init.xavier_normal(m.weight)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:50: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with Xavier initializer\n",
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "if cfg.INITIALIZER == 'Xavier':\n",
    "    model.apply(model.weights_init)\n",
    "    nn.init.normal_(model.fc[2].weight.data,0,0.01)\n",
    "    nn.init.constant_(model.fc[2].bias.data,0)\n",
    "    print('Initialized with Xavier initializer')\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    print('Using CUDA!')\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)\n",
    "\n",
    "replay_initial = cfg.REPLAY_INIT\n",
    "replay_buffer = ReplayBuffer(cfg.REPLAY_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy maximum: 1.792\n"
     ]
    }
   ],
   "source": [
    "entropy_threshold = np.log(env.action_space.n) * cfg.Q_ENTROPY_THRESHOLD\n",
    "qe_max = np.log(env.action_space.n)\n",
    "print('entropy maximum: %.3f' %(qe_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Entropy(p):\n",
    "    p1 = np.exp(p, dtype=np.float64) / np.sum(np.exp(p), dtype=np.float64)\n",
    "    return -np.sum(p1*np.log(p1), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7913491226700762"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=np.array([637.17, 637.13, 637.14, 637.10, 637.19, 637.15])\n",
    "entropy(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame:  6000 (0.20%), q_entropy: 1.791, (99.98% of max)  q_values: 637.13, 637.15, 637.14, 637.11, 637.11, 637.19"
     ]
    }
   ],
   "source": [
    "num_frames = cfg.NUM_FRAMES\n",
    "batch_size = cfg.BATCH_SIZE\n",
    "gamma      = cfg.GAMMA\n",
    "episode = 0\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state_traj = []\n",
    "q_value_traj = []\n",
    "\n",
    "episode_state = []\n",
    "episode_q_val = []\n",
    "\n",
    "\"\"\"\n",
    "fig = plt.figure(figsize=[10,5])\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\"\"\"\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    q_values = model.predict(state).data.cpu().numpy()[0]\n",
    "    q_values -= np.max(q_values)\n",
    "    q_values += np.log(np.finfo(np.float64).max/q_values.shape[-1])*0.9\n",
    "    q_values = q_values.astype(np.float64)\n",
    "\n",
    "    #q_values /= (np.max(q_values)+1e-6)\n",
    "    #q_values *= 10\n",
    "    #q_values = np.clip(q_values,-10,10)\n",
    "    q_entropy = Entropy(q_values)\n",
    "    \n",
    "    \n",
    "    if frame_idx % 100 == 0:\n",
    "        QQ= q_values#.data.cpu().numpy()[0]\n",
    "        \"\"\"\n",
    "        ax.clear()\n",
    "        plt.ylim(0,800)\n",
    "        ax.bar(range(env.action_space.n), QQ)\n",
    "        fig.canvas.draw()\n",
    "        \"\"\"\n",
    "        print('\\rframe: %5d (%.2f%%), q_entropy: %.3f, (%.2f%% of max)  q_values: %.2f, %.2f, %.2f, %.2f, %.2f, %.2f'\n",
    "        %(frame_idx, (frame_idx/num_frames*100), q_entropy, (q_entropy/qe_max)*100, QQ[0], QQ[1], QQ[2], QQ[3], QQ[4], QQ[5]),end='')\n",
    "\n",
    "    if q_entropy > entropy_threshold:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = model.act(state=state, epsilon=0)\n",
    "    \n",
    "    \n",
    "    if episode % 20 == 0:\n",
    "        episode_state.append(state)\n",
    "        episode_q_val.append(q_values)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        if episode % 20 == 0:\n",
    "            state_traj.append(episode_state)\n",
    "            q_value_traj.append(episode_q_val)\n",
    "        \n",
    "        episode += 1\n",
    "        episode_state = []\n",
    "        episode_q_val = []\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    #print('frame_idx: ', frame_idx, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from common.save_file import *\n",
    "\n",
    "model_dir = \"model\"\n",
    "var_dir = \"var\"\n",
    "name = \"qentropy_\" + env_id\n",
    "\n",
    "save_model(model, model_dir, name)\n",
    "\n",
    "var_dict = {\n",
    "            \"all_rewards\": all_rewards,\n",
    "            \"losses\": losses,\n",
    "            \"state_traj\": state_traj,\n",
    "            \"q_value_traj\": q_value_traj,\n",
    "           }\n",
    "\n",
    "save_variable(name, var_dir, var_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
