{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_id = \"Boxing-v0\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><hr></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "    \n",
    "    def predict(self, state):\n",
    "        state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "        q_value = self.forward(state)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "replay_initial = 20000\n",
    "replay_buffer = ReplayBuffer(300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Epsilon greedy exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x124df5b38>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 150000 \n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "plt.plot([epsilon_by_frame(i) for i in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    p1 = np.exp(p) / np.sum(np.exp(p))\n",
    "    return -sum(p1*np.log(p1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Episode: 2\n",
      "Episode: 3\n",
      "Episode: 4\n",
      "Episode: 5\n",
      "Episode: 6\n",
      "Episode: 7\n",
      "Episode: 8\n",
      "Episode: 9\n",
      "Episode: 10\n",
      "Episode: 11\n",
      "Episode: 12\n",
      "Episode: 13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e19b1543ac41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mepisode_q_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mObservationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEpisodicLifeEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-Adventure/common/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFireResetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_frames = 3000000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "episode = 0\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state_traj = []\n",
    "q_value_traj = []\n",
    "q_entropy_traj = []\n",
    "\n",
    "episode_state = []\n",
    "episode_q_val = []\n",
    "episode_q_entropy = []\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    q_values = model.predict(state)\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        episode_state.append(state)\n",
    "        episode_q_val.append(q_values)\n",
    "        q_entropy = entropy(q_values)\n",
    "        episode_q_entropy.append(q_entropy)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        if episode % 20 == 0:\n",
    "            state_traj.append(episode_state)\n",
    "            q_value_traj.append(episode_q_val)\n",
    "            q_entropy_traj.append(episode_q_entropy)\n",
    "        \n",
    "        episode += 1\n",
    "        episode_state = []\n",
    "        episode_q_val = []\n",
    "        episode_q_entropy = []\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minzy/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type CnnDQN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 557\n"
     ]
    }
   ],
   "source": [
    "from common.save_file import *\n",
    "\n",
    "model_dir = \"model\"\n",
    "var_dir = \"var\"\n",
    "name = \"egreedy_\" + env_id\n",
    "\n",
    "save_model(model, model_dir, name)\n",
    "\n",
    "var_dict = {\n",
    "            \"all_rewards\": all_rewards,\n",
    "            \"losses\": losses,\n",
    "            \"state_traj\": state_traj,\n",
    "            \"q_value_traj\": q_value_traj,\n",
    "            \"q_entropy\": q_entropy_traj\n",
    "           }\n",
    "\n",
    "save_variable(name, var_dir, var_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
